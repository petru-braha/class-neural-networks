{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "np.random.seed(74)\n",
        "\n",
        "EPOCHS = 60\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01\n",
        "DROPOUT_RATE = 0.3\n",
        "L2_LAMBDA = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivat(a):\n",
        "    return a * (1.0 - a)\n",
        "\n",
        "def one_hot_encode(y, n_classes=10):\n",
        "    n = len(y)\n",
        "    one_hot = np.zeros((n, n_classes))\n",
        "    one_hot[np.arange(n), y] = 1\n",
        "    return one_hot\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(a2, y, weights_input_hidden, weights_hidden_output, l2_lambda=0.0):\n",
        "    m = a2.shape[0]\n",
        "    y_encoded = one_hot_encode(y, a2.shape[1])\n",
        "    eps = 1e-12\n",
        "    loss = -np.sum(y_encoded * np.log(a2 + eps)) / m\n",
        "\n",
        "    if l2_lambda and l2_lambda > 0:\n",
        "        loss += 0.5 * l2_lambda * (np.sum(weights_input_hidden**2) + np.sum(weights_hidden_output**2))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_weights(input_size, hidden_size, output_size):\n",
        "    weights_input_hidden = np.random.randn(input_size, hidden_size) * np.sqrt(1.0 / input_size)\n",
        "    bias_hidden = np.zeros((1, hidden_size))\n",
        "    weights_hidden_output = np.random.randn(hidden_size, output_size) * np.sqrt(1.0 / hidden_size)\n",
        "    bias_output = np.zeros((1, output_size))\n",
        "    return weights_input_hidden, bias_hidden, weights_hidden_output, bias_output\n",
        "\n",
        "def forward(x_batch, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=True):\n",
        "    z1 = np.dot(x_batch, weights_input_hidden) + bias_hidden\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    dropout_mask = None\n",
        "    # Check if the current state could be training\n",
        "    if training and dropout_rate > 0:\n",
        "        dropout_mask = (np.random.rand(*a1.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "        a1 = a1 * dropout_mask\n",
        "    \n",
        "    # Compute the next layer output\n",
        "    z2 = np.dot(a1, weights_hidden_output) + bias_output\n",
        "    a2 = softmax(z2)\n",
        "    \n",
        "    return z1, a1, z2, a2, dropout_mask\n",
        "\n",
        "def backward(x_batch, y_batch, z1, a1, z2, a2, weights_input_hidden, weights_hidden_output, dropout_mask, l2_lambda=0.0001):\n",
        "    m = x_batch.shape[0]\n",
        "    y_encoded = one_hot_encode(y_batch, a2.shape[1])\n",
        "    \n",
        "    # Compute diff between target and prediction\n",
        "    delta2 = a2 - y_encoded\n",
        "\n",
        "    # Normalize l2 for computing gradients by using the next (second) layer\n",
        "    dW2 = np.dot(a1.T, delta2) / m + l2_lambda * weights_hidden_output\n",
        "    db2 = np.sum(delta2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Backpropagate the error to the previous layer\n",
        "    delta1 = np.dot(delta2, weights_hidden_output.T) * sigmoid_derivat(a1)\n",
        "    if dropout_mask is not None:\n",
        "        delta1 = delta1 * dropout_mask\n",
        "\n",
        "    # Normalize l2 for computing gradients by using the current (first) layer\n",
        "    dW1 = np.dot(x_batch.T, delta1) / m + l2_lambda * weights_input_hidden\n",
        "    db1 = np.sum(delta1, axis=0, keepdims=True) / m\n",
        "    \n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def compute_accuracy(X, y, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output):\n",
        "    _, _, _, a2, _ = forward(X, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n",
        "    predictions = np.argmax(a2, axis=1)\n",
        "    return np.mean(predictions == y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_file_path = '/kaggle/input/fii-nn-2025-homework-3/extended_mnist_train.pkl'\n",
        "test_file_path = '/kaggle/input/fii-nn-2025-homework-3/extended_mnist_test.pkl'\n",
        "\n",
        "with open(train_file_path, 'rb') as fp:\n",
        "    train_dataset = pickle.load(fp)\n",
        "with open(test_file_path, 'rb') as fp:\n",
        "    test_dataset = pickle.load(fp)\n",
        "\n",
        "# Preprocess\n",
        "train_images_flat = []\n",
        "train_image_labels = []\n",
        "for image, label in train_dataset:\n",
        "    train_images_flat.append(image.flatten() / 255.0)\n",
        "    train_image_labels.append(label)\n",
        "\n",
        "test_images_flat = []\n",
        "for image, label in test_dataset:\n",
        "    test_images_flat.append(image.flatten() / 255.0)\n",
        "\n",
        "train_features_full = np.array(train_images_flat)\n",
        "train_labels_full = np.array(train_image_labels)\n",
        "test_features = np.array(test_images_flat)\n",
        "\n",
        "# Split\n",
        "split = int(0.9 * len(train_features_full))\n",
        "train_features = train_features_full[:split]\n",
        "train_image_labels = train_labels_full[:split]\n",
        "validation_features = train_features_full[split:]\n",
        "validation_labels = train_labels_full[split:]\n",
        "\n",
        "# Initialize\n",
        "weights_input_hidden, bias_hidden, weights_hidden_output, bias_output = initialize_weights(784, 100, 10)\n",
        "\n",
        "n_samples = len(train_features)\n",
        "n_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE  # Handle last batch\n",
        "\n",
        "best_val_acc = 0\n",
        "best_weights = None\n",
        "count_lr_changes = 0\n",
        "lr = LEARNING_RATE\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracies(epoch, epoch_loss):\n",
        "    train_acc = compute_accuracy(train_features, train_image_labels, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
        "    val_acc = compute_accuracy(validation_features, validation_labels, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
        "        \n",
        "    _, _, _, a2_val, _ = forward(validation_features, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n",
        "    val_loss = cross_entropy_loss(a2_val, validation_labels, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n",
        "        \n",
        "    print(f'epoch index {epoch+1:3d}/{EPOCHS} | '\n",
        "          f'train_dataset acc: {train_acc:.4f}  val acc: {val_acc:.4f} | '\n",
        "          f'train_dataset loss: {epoch_loss:.4f}  val loss: {val_loss:.4f} | '\n",
        "          f'lr: {lr:.4f}  '\n",
        "          f'time: {time.time() - start_time:.1f}s')\n",
        "    return val_acc\n",
        "\n",
        "def update_best_weights(val_acc):\n",
        "    global best_val_acc\n",
        "    global best_weights\n",
        "    global count_lr_changes\n",
        "    global lr\n",
        "\n",
        "    # Check whether the model has improved with the current learning rate\n",
        "    if val_acc <= best_val_acc:\n",
        "        count_lr_changes += 1\n",
        "        if count_lr_changes >= 3:\n",
        "            lr *= 0.7\n",
        "            count_lr_changes = 0\n",
        "            print(f'reduced learning rate to {lr:.6f}')\n",
        "        return\n",
        "\n",
        "    # Update the global weights\n",
        "    best_val_acc = val_acc\n",
        "    best_weights = (weights_input_hidden.copy(), bias_hidden.copy(), weights_hidden_output.copy(), bias_output.copy())\n",
        "    print(f'new model validation accuracy: {best_val_acc:.4f}')\n",
        "    count_lr_changes = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training\n",
        "print(f'Train {n_samples} samples using {BATCH_SIZE} batches per epoch')\n",
        "print(f'Normalization parameters: dropout={DROPOUT_RATE}, L2={L2_LAMBDA}')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Initial suffle\n",
        "    index = np.random.permutation(n_samples)\n",
        "    x_shuffled = train_features[index]\n",
        "    y_shuffled = train_image_labels[index]\n",
        "    \n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        begin = i * BATCH_SIZE\n",
        "        end = min(begin + BATCH_SIZE, n_samples)\n",
        "        \n",
        "        x_batch = x_shuffled[begin:end]\n",
        "        y_batch = y_shuffled[begin:end]\n",
        "        \n",
        "        z1, a1, z2, a2, dropout_mask = forward(\n",
        "            x_batch, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, \n",
        "            dropout_rate=DROPOUT_RATE, training=True\n",
        "        )\n",
        "        \n",
        "        batch_loss = cross_entropy_loss(a2, y_batch, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n",
        "        epoch_loss += batch_loss * len(x_batch)\n",
        "        \n",
        "        dW1, db1, dW2, db2 = backward(\n",
        "            x_batch, y_batch, z1, a1, z2, a2, weights_input_hidden, weights_hidden_output, dropout_mask, L2_LAMBDA\n",
        "        )\n",
        "        \n",
        "        # Compute new weights\n",
        "        weights_input_hidden -= lr * dW1\n",
        "        bias_hidden -= lr * db1\n",
        "        weights_hidden_output -= lr * dW2\n",
        "        bias_output -= lr * db2\n",
        "    \n",
        "    epoch_loss /= n_samples\n",
        "    \n",
        "    # Print extra statistics\n",
        "    if epoch % 5 != 0 and epoch != EPOCHS - 1:\n",
        "        continue\n",
        "    val_acc = compute_accuracies(epoch, epoch_loss)\n",
        "    update_best_weights(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for optimal results\n",
        "if best_weights is not None:\n",
        "    weights_input_hidden, bias_hidden, weights_hidden_output, bias_output = best_weights\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "train_acc_final = compute_accuracy(train_features_full, train_labels_full, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
        "val_acc_final = compute_accuracy(validation_features, validation_labels, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n",
        "\n",
        "_, _, _, a2_test, _ = forward(test_features, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n",
        "predictions = np.argmax(a2_test, axis=1)\n",
        "\n",
        "_, _, _, a2_train, _ = forward(train_features_full, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n",
        "_, _, _, a2_val, _ = forward(validation_features, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n",
        "train_loss = cross_entropy_loss(a2_train, train_labels_full, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n",
        "val_loss = cross_entropy_loss(a2_val, validation_labels, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n",
        "\n",
        "print(f'results\\n')\n",
        "print(f'training accuracy: {train_acc_final*100:.2f}%')\n",
        "print(f'model validation accuracy: {val_acc_final*100:.2f}%')\n",
        "print(f'training loss: {train_loss:.4f}')\n",
        "print(f'validation loss loss: {val_loss:.4f}')\n",
        "print(f'time: {training_time:.2f}s ({training_time/60:.2f}min)')\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'target': predictions.astype(int)\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print('submission done')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
