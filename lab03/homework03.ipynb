{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":117046,"databundleVersionId":13980102,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pickle\nimport pandas as pd\nimport time\n\nnp.random.seed(74)\n\nEPOCHS = 60\nBATCH_SIZE = 128\nLEARNING_RATE = 0.01\nDROPOUT_RATE = 0.3\nL2_LAMBDA = 0.0001","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:50:33.121930Z","iopub.execute_input":"2025-11-27T17:50:33.122251Z","iopub.status.idle":"2025-11-27T17:50:35.063429Z","shell.execute_reply.started":"2025-11-27T17:50:33.122213Z","shell.execute_reply":"2025-11-27T17:50:35.062531Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def sigmoid(z):\n    return np.where(z < 0, 0, z)\n    z = np.clip(z, -500, 500)\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef sigmoid_derivat(a):\n    return (a > 0).astype(np.float32)\n    return a * (1.0 - a)\n\ndef one_hot_encode(y, n_classes=10):\n    n = len(y)\n    one_hot = np.zeros((n, n_classes))\n    one_hot[np.arange(n), y] = 1\n    return one_hot\n\ndef softmax(z):\n    z = z - np.max(z, axis=1, keepdims=True)\n    exp_z = np.exp(z)\n    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\ndef cross_entropy_loss(a2, y, weights_input_hidden, weights_hidden_output, l2_lambda=0.0):\n    m = a2.shape[0]\n    y_encoded = one_hot_encode(y, a2.shape[1])\n    eps = 1e-12\n    loss = -np.sum(y_encoded * np.log(a2 + eps)) / m\n\n    if l2_lambda and l2_lambda > 0:\n        loss += 0.5 * l2_lambda * (np.sum(weights_input_hidden**2) + np.sum(weights_hidden_output**2))\n    return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:50:35.065116Z","iopub.execute_input":"2025-11-27T17:50:35.065504Z","iopub.status.idle":"2025-11-27T17:50:35.074691Z","shell.execute_reply.started":"2025-11-27T17:50:35.065464Z","shell.execute_reply":"2025-11-27T17:50:35.073666Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def initialize_weights(input_size, hidden_size, output_size):\n    weights_input_hidden = np.random.randn(input_size, hidden_size) * np.sqrt(1.0 / input_size)\n    bias_hidden = np.zeros((1, hidden_size))\n    weights_hidden_output = np.random.randn(hidden_size, output_size) * np.sqrt(1.0 / hidden_size)\n    bias_output = np.zeros((1, output_size))\n    return weights_input_hidden, bias_hidden, weights_hidden_output, bias_output\n\ndef forward(x_batch, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=True):\n    z1 = np.dot(x_batch, weights_input_hidden) + bias_hidden\n    a1 = sigmoid(z1)\n\n    dropout_mask = None\n    # Check if the current state could be training\n    if training and dropout_rate > 0:\n        dropout_mask = (np.random.rand(*a1.shape) > dropout_rate) / (1 - dropout_rate)\n        a1 = a1 * dropout_mask\n    \n    # Compute the next layer output\n    z2 = np.dot(a1, weights_hidden_output) + bias_output\n    a2 = softmax(z2)\n    \n    return z1, a1, z2, a2, dropout_mask\n\ndef backward(x_batch, y_batch, z1, a1, z2, a2, weights_input_hidden, weights_hidden_output, dropout_mask, l2_lambda=0.0001):\n    m = x_batch.shape[0]\n    y_encoded = one_hot_encode(y_batch, a2.shape[1])\n    \n    # Compute diff between target and prediction\n    delta2 = a2 - y_encoded\n\n    # Normalize l2 for computing gradients by using the next (second) layer\n    dW2 = np.dot(a1.T, delta2) / m + l2_lambda * weights_hidden_output\n    db2 = np.sum(delta2, axis=0, keepdims=True) / m\n\n    # Backpropagate the error to the previous layer\n    delta1 = np.dot(delta2, weights_hidden_output.T) * sigmoid_derivat(a1)\n    if dropout_mask is not None:\n        delta1 = delta1 * dropout_mask\n\n    # Normalize l2 for computing gradients by using the current (first) layer\n    dW1 = np.dot(x_batch.T, delta1) / m + l2_lambda * weights_input_hidden\n    db1 = np.sum(delta1, axis=0, keepdims=True) / m\n    \n    return dW1, db1, dW2, db2\n\ndef compute_accuracy(X, y, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output):\n    _, _, _, a2, _ = forward(X, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n    predictions = np.argmax(a2, axis=1)\n    return np.mean(predictions == y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:50:35.075784Z","iopub.execute_input":"2025-11-27T17:50:35.076648Z","iopub.status.idle":"2025-11-27T17:50:35.101809Z","shell.execute_reply.started":"2025-11-27T17:50:35.076612Z","shell.execute_reply":"2025-11-27T17:50:35.100796Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Load data\ntrain_file_path = '/kaggle/input/fii-nn-2025-homework-3/extended_mnist_train.pkl'\ntest_file_path = '/kaggle/input/fii-nn-2025-homework-3/extended_mnist_test.pkl'\n\nwith open(train_file_path, 'rb') as fp:\n    train_dataset = pickle.load(fp)\nwith open(test_file_path, 'rb') as fp:\n    test_dataset = pickle.load(fp)\n\n# Preprocess\ntrain_images_flat = []\ntrain_image_labels = []\nfor image, label in train_dataset:\n    train_images_flat.append(image.flatten() / 255.0)\n    train_image_labels.append(label)\n\ntest_images_flat = []\nfor image, label in test_dataset:\n    test_images_flat.append(image.flatten() / 255.0)\n\ntrain_features_full = np.array(train_images_flat)\ntrain_labels_full = np.array(train_image_labels)\ntest_features = np.array(test_images_flat)\n\n# Split\nsplit = int(0.9 * len(train_features_full))\ntrain_features = train_features_full[:split]\ntrain_image_labels = train_labels_full[:split]\nvalidation_features = train_features_full[split:]\nvalidation_labels = train_labels_full[split:]\n\n# Initialize\nweights_input_hidden, bias_hidden, weights_hidden_output, bias_output = initialize_weights(784, 100, 10)\n\nn_samples = len(train_features)\nn_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE  # Handle last batch\n\nbest_val_acc = 0\nbest_weights = None\ncount_lr_changes = 0\nlr = LEARNING_RATE\nstart_time = time.time()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:50:35.102822Z","iopub.execute_input":"2025-11-27T17:50:35.103121Z","iopub.status.idle":"2025-11-27T17:50:36.950407Z","shell.execute_reply.started":"2025-11-27T17:50:35.103098Z","shell.execute_reply":"2025-11-27T17:50:36.949486Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def compute_accuracies(epoch, epoch_loss):\n    train_acc = compute_accuracy(train_features, train_image_labels, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n    val_acc = compute_accuracy(validation_features, validation_labels, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n        \n    _, _, _, a2_val, _ = forward(validation_features, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n    val_loss = cross_entropy_loss(a2_val, validation_labels, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n        \n    print(f'epoch index {epoch+1:3d}/{EPOCHS} | '\n          f'train_dataset acc: {train_acc:.4f}  val acc: {val_acc:.4f} | '\n          f'train_dataset loss: {epoch_loss:.4f}  val loss: {val_loss:.4f} | '\n          f'lr: {lr:.4f}  '\n          f'time: {time.time() - start_time:.1f}s')\n    return val_acc\n\ndef update_best_weights(val_acc):\n    global best_val_acc\n    global best_weights\n    global count_lr_changes\n    global lr\n\n    # Check whether the model has improved with the current learning rate\n    if val_acc <= best_val_acc:\n        count_lr_changes += 1\n        if count_lr_changes >= 3:\n            lr *= 0.7\n            count_lr_changes = 0\n            print(f'reduced learning rate to {lr:.6f}')\n        return\n\n    # Update the global weights\n    best_val_acc = val_acc\n    best_weights = (weights_input_hidden.copy(), bias_hidden.copy(), weights_hidden_output.copy(), bias_output.copy())\n    print(f'new model validation accuracy: {best_val_acc:.4f}')\n    count_lr_changes = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:50:36.951370Z","iopub.execute_input":"2025-11-27T17:50:36.951705Z","iopub.status.idle":"2025-11-27T17:50:36.960167Z","shell.execute_reply.started":"2025-11-27T17:50:36.951679Z","shell.execute_reply":"2025-11-27T17:50:36.959192Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Training\nprint(f'Train {n_samples} samples using {BATCH_SIZE} batches per epoch')\nprint(f'Normalization parameters: dropout={DROPOUT_RATE}, L2={L2_LAMBDA}')\n\nfor epoch in range(EPOCHS):\n    # Initial suffle\n    index = np.random.permutation(n_samples)\n    x_shuffled = train_features[index]\n    y_shuffled = train_image_labels[index]\n    \n    epoch_loss = 0\n\n    for i in range(n_batches):\n        begin = i * BATCH_SIZE\n        end = min(begin + BATCH_SIZE, n_samples)\n        \n        x_batch = x_shuffled[begin:end]\n        y_batch = y_shuffled[begin:end]\n        \n        z1, a1, z2, a2, dropout_mask = forward(\n            x_batch, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, \n            dropout_rate=DROPOUT_RATE, training=True\n        )\n        \n        batch_loss = cross_entropy_loss(a2, y_batch, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n        epoch_loss += batch_loss * len(x_batch)\n        \n        dW1, db1, dW2, db2 = backward(\n            x_batch, y_batch, z1, a1, z2, a2, weights_input_hidden, weights_hidden_output, dropout_mask, L2_LAMBDA\n        )\n        \n        # Compute new weights\n        weights_input_hidden -= lr * dW1\n        bias_hidden -= lr * db1\n        weights_hidden_output -= lr * dW2\n        bias_output -= lr * db2\n    \n    epoch_loss /= n_samples\n    \n    # Print extra statistics\n    if epoch % 5 != 0 and epoch != EPOCHS - 1:\n        continue\n    val_acc = compute_accuracies(epoch, epoch_loss)\n    update_best_weights(val_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:50:36.961165Z","iopub.execute_input":"2025-11-27T17:50:36.961491Z","iopub.status.idle":"2025-11-27T17:51:45.120099Z","shell.execute_reply.started":"2025-11-27T17:50:36.961437Z","shell.execute_reply":"2025-11-27T17:51:45.119259Z"}},"outputs":[{"name":"stdout","text":"Train 54000 samples using 128 batches per epoch\nNormalization parameters: dropout=0.3, L2=0.0001\nepoch index   1/60 | train_dataset acc: 0.8222  val acc: 0.8607 | train_dataset loss: 1.5212  val loss: 0.8457 | lr: 0.0100  time: 2.0s\nnew model validation accuracy: 0.8607\nepoch index   6/60 | train_dataset acc: 0.8969  val acc: 0.9202 | train_dataset loss: 0.4736  val loss: 0.3228 | lr: 0.0100  time: 7.9s\nnew model validation accuracy: 0.9202\nepoch index  11/60 | train_dataset acc: 0.9128  val acc: 0.9318 | train_dataset loss: 0.3862  val loss: 0.2617 | lr: 0.0100  time: 13.5s\nnew model validation accuracy: 0.9318\nepoch index  16/60 | train_dataset acc: 0.9234  val acc: 0.9398 | train_dataset loss: 0.3408  val loss: 0.2294 | lr: 0.0100  time: 19.1s\nnew model validation accuracy: 0.9398\nepoch index  21/60 | train_dataset acc: 0.9310  val acc: 0.9462 | train_dataset loss: 0.3073  val loss: 0.2068 | lr: 0.0100  time: 24.7s\nnew model validation accuracy: 0.9462\nepoch index  26/60 | train_dataset acc: 0.9374  val acc: 0.9518 | train_dataset loss: 0.2829  val loss: 0.1898 | lr: 0.0100  time: 30.3s\nnew model validation accuracy: 0.9518\nepoch index  31/60 | train_dataset acc: 0.9418  val acc: 0.9567 | train_dataset loss: 0.2642  val loss: 0.1771 | lr: 0.0100  time: 35.9s\nnew model validation accuracy: 0.9567\nepoch index  36/60 | train_dataset acc: 0.9454  val acc: 0.9608 | train_dataset loss: 0.2487  val loss: 0.1661 | lr: 0.0100  time: 41.5s\nnew model validation accuracy: 0.9608\nepoch index  41/60 | train_dataset acc: 0.9488  val acc: 0.9630 | train_dataset loss: 0.2371  val loss: 0.1579 | lr: 0.0100  time: 47.1s\nnew model validation accuracy: 0.9630\nepoch index  46/60 | train_dataset acc: 0.9523  val acc: 0.9647 | train_dataset loss: 0.2272  val loss: 0.1510 | lr: 0.0100  time: 52.6s\nnew model validation accuracy: 0.9647\nepoch index  51/60 | train_dataset acc: 0.9543  val acc: 0.9657 | train_dataset loss: 0.2171  val loss: 0.1449 | lr: 0.0100  time: 58.2s\nnew model validation accuracy: 0.9657\nepoch index  56/60 | train_dataset acc: 0.9564  val acc: 0.9657 | train_dataset loss: 0.2090  val loss: 0.1396 | lr: 0.0100  time: 63.7s\nepoch index  60/60 | train_dataset acc: 0.9584  val acc: 0.9665 | train_dataset loss: 0.2035  val loss: 0.1366 | lr: 0.0100  time: 68.2s\nnew model validation accuracy: 0.9665\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Check for optimal results\nif best_weights is not None:\n    weights_input_hidden, bias_hidden, weights_hidden_output, bias_output = best_weights\n\ntraining_time = time.time() - start_time\n\ntrain_acc_final = compute_accuracy(train_features_full, train_labels_full, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\nval_acc_final = compute_accuracy(validation_features, validation_labels, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)\n\n_, _, _, a2_test, _ = forward(test_features, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\npredictions = np.argmax(a2_test, axis=1)\n\n_, _, _, a2_train, _ = forward(train_features_full, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\n_, _, _, a2_val, _ = forward(validation_features, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output, dropout_rate=0, training=False)\ntrain_loss = cross_entropy_loss(a2_train, train_labels_full, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\nval_loss = cross_entropy_loss(a2_val, validation_labels, weights_input_hidden, weights_hidden_output, L2_LAMBDA)\n\nprint(f'results\\n')\nprint(f'training accuracy: {train_acc_final*100:.2f}%')\nprint(f'model validation accuracy: {val_acc_final*100:.2f}%')\nprint(f'training loss: {train_loss:.4f}')\nprint(f'validation loss loss: {val_loss:.4f}')\nprint(f'time: {training_time:.2f}s ({training_time/60:.2f}min)')\n\nsubmission = pd.DataFrame({\n    'ID': range(len(predictions)),\n    'target': predictions.astype(int)\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint('submission done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T17:51:45.121995Z","iopub.execute_input":"2025-11-27T17:51:45.122256Z","iopub.status.idle":"2025-11-27T17:51:45.838667Z","shell.execute_reply.started":"2025-11-27T17:51:45.122236Z","shell.execute_reply":"2025-11-27T17:51:45.837763Z"}},"outputs":[{"name":"stdout","text":"results\n\ntraining accuracy: 95.92%\nmodel validation accuracy: 96.65%\ntraining loss: 0.1548\nvalidation loss loss: 0.1366\ntime: 68.18s (1.14min)\nsubmission done\n","output_type":"stream"}],"execution_count":7}]}